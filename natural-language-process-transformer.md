# Natural Language Processing Transformer


|No.  |Model Name |Title |Links |Pub. | Organization| Release Time | 
|-----|:-----:|:-----:|:-----:|:--------:|:---:|:-------:|
|1|BERT|BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding |[paper](https://arxiv.org/abs/1810.04805) [code](https://github.com/google-research/bert) |__NAACL 2019__|Google|Oct 2018|
|2|GPT3|Language Models are Few-Shot Learners|[paper](https://arxiv.org/abs/2005.14165) | __NeuRIPS 2020__ | OpenAI | May 2020|
|3|GPT2|Language Models are Unsupervised Multitask Learners |[paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [code](https://github.com/openai/gpt-2)|__arXiv__ | OpenAI | Feb 2019|
|4| RoBERTa | RoBERTa: A Robustly Optimized BERT Pretraining Approach | [paper](https://arxiv.org/abs/1907.11692) | __arXiv__ |Facebook AI | Jul 2019|
