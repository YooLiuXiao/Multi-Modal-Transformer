# Efficient Transformer

|No.  |Model Name |Title |Links |Pub. | Organization| Release Time |
|-----|:-----:|:-----:|:-----:|:--------:|:---:|:-------:|
|1|HiBERT |HIBERT: Document Level Pre-training of Hierarchical BidirectionalTransformers for Document Summarization |[paper](https://arxiv.org/pdf/1905.06566.pdf) |__ACL 2019__|Microsoft Research Asia|16 May 2019|
|2|star transformer |[paper](https://www.aclweb.org/anthology/N19-1133.pdf) |__NAACL 2019__|Shanghai Key Laboratory of Intelligent Information Processing, Fudan University|25 Feb 2019|
|3|ETC |ETC: Encoding Long and Structured Inputs in Transformers |[paper](https://www.aclweb.org/anthology/2020.emnlp-main.19.pdf) |__EMNLP 2020__|Google AI|16 November 2020|
|4|BP-Transformer |BP-Transformer: Modelling Long-Range Context via Binary Partitioning |[paper](https://arxiv.org/pdf/1911.04070.pdf)|--arXiv|AWS Shanghai AI Lab|11 November 2019||









## Common Datasets
|No. |Dataset Name |Download Link|
|-----|:-----:|:-----:|
r
