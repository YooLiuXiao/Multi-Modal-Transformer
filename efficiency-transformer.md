# Efficient Transformer

|No.  |Model Name |Title |Links |Pub. | Organization| Release Time |
|-----|:-----:|:-----:|:-----:|:--------:|:---:|:-------:|
|1|HiBERT |HIBERT: Document Level Pre-training of Hierarchical BidirectionalTransformers for Document Summarization |[paper](https://arxiv.org/pdf/1905.06566.pdf) |__ACL 2019__|Microsoft Research Asia|16 May 2019|
|2|star transformer|Star Transformer |[paper](https://www.aclweb.org/anthology/N19-1133.pdf) |__NAACL 2019__|Shanghai Key Laboratory of Intelligent Information Processing, Fudan University|25 Feb 2019|
|3|ETC |ETC: Encoding Long and Structured Inputs in Transformers |[paper](https://www.aclweb.org/anthology/2020.emnlp-main.19.pdf) |__EMNLP 2020__|Google AI|16 November 2020|
|4|BP-Transformer |BP-Transformer: Modelling Long-Range Context via Binary Partitioning |[paper](https://arxiv.org/pdf/1911.04070.pdf) [code](https://github.com/yzh119/BPT)|__arXiv__|AWS Shanghai AI Lab|11 November 2019|
|5|Routing Transformer |Efficient Content-Based Sparse Attention with Routing Transformers |[paper](https://openreview.net/forum?id=B1gjs6EtDr) [code](https://github.com/lucidrains/routing-transformer)|__ICLR 2020__|Google AI|1 Februray 2021|
|7|Compressive Transformer |Compressive Transformers for Long-Range Sequence Modelling |[paper](https://openreview.net/pdf?id=SylKikSYDH) [code](https://github.com/lucidrains/compressive-transformer-pytorch)|__ICLR 2020__|Deep Mind|25 Sep 2019|










## Common Datasets
|No. |Dataset Name |Download Link|
|-----|:-----:|:-----:|
r
