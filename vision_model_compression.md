# Compressed Transformer

|No.  |Model Name |Title |Links |Pub. | Organization| Release Time |
|-----|:-----:|:-----:|:-----:|:--------:|:---:|:-------:|
|1| VTP |Vision Transformer Pruning |[paper](https://arxiv.org/pdf/2104.08500.pdf) |__KDD 2021 workshop__|Westlake University|14 Aug 2021|
|2| IA-RED2 | IA-RED2 : Interpretability-Aware Redundancy Reduction for Vision Transformers | [paper](https://proceedings.neurips.cc/paper/2021/hash/d072677d210ac4c03ba046120f0802ec-Abstract.html) [code](http://people.csail.mit.edu/bpan/ia-red/) | __NeurIPS 2021__ | MIT| 23 Jun 2021|
|3| DynamicViT| DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification | __NeurIPS 2021__| [paper](https://arxiv.org/pdf/2106.02034.pdf) [code](https://github.com/raoyongming/DynamicViT) |  Tsinghua University| 26 Oct 2021|
|4|  Evo-ViT| Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer| [paper](https://arxiv.org/pdf/2108.01390.pdf) [code](https://github.com/YifanXu74/Evo-ViT)|__arXiv__|Chinese Academy of Sciences |6 Dec 2021|
|5| - |Patch Slimming for Efficient Vision Transformers| [paper](https://arxiv.org/pdf/2106.02852.pdf) |__arXiv__| Peking University|5 Jun 2021|
|6|-| Chasing Sparsity in Vision Transformers: An End-to-End Exploration| [paper](https://arxiv.org/pdf/2106.04533.pdf) [code](https://github.com/VITA-Group/SViTE) | __arXiv__| University of Texas at Austin| 22 Oct 2021|
|7|DeIT| Training data-efficient image transformers & distillation through attention | [paper](https://arxiv.org/pdf/2012.12877.pdf) | __ICML 2021__|Facebook | 15 Jan 2021|
|8| -|Post-Training Quantization for Vision Transformer| [paper](https://arxiv.org/abs/2106.14156) | __NeurIPS 2021__| Peking University| 27 Jun 2021|
|9| -| Multi-Dimensional Model Compression of Vision Transformer | [paper](https://arxiv.org/pdf/2201.00043.pdf) | __arXiv__| Princeton University |31 Dec 2021|
|10|-| Patch Slimming for Efficient Vision Transformers|[paper](https://arxiv.org/pdf/2106.02852.pdf) | __arXiv__ |Peking University|5 Jun 2021|
|11|-| Chasing Sparsity in Vision Transformers: An End-to-End Exploration| [paper](https://arxiv.org/pdf/2106.04533.pdf) [code](https://github.com/VITA-Group/SViTE)| NeurIPS 2021 | University of Texas at Austin|22 Oct 2021|
