## Image Classification

|No.  |Model Name |Title |Links |Pub. | Organization| Release Time | 
|-----|:-----:|:-----:|:-----:|:--------:|:---:|:-------:|
|1|ViT |An image is worth 16 * 16 words: transformers for image recognition at scale |[paper]( https://arxiv.org/pdf/2010.11929.pdf) [code]( https://github.com/rwightman/pytorch-image-models) |__ICLR 2021__|Google Brain|22 Oct 2020|
|2|LeViT |LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference |[paper](https://arxiv.org/abs/2104.01136)  |__arXiv__|/|2 Apr 2021|
|3|Swin Transformer |Swin Transformer: Hierarchical Vision Transformer using Shifted Windows |[paper](https://arxiv.org/pdf/2103.14030.pdf) [code](https://github.com/microsoft/Swin-Transformer)  |__arXiv__|MSRA|25 Mar 2021|
|4|DeiT Transformer |Training data-efficient image transformers& distillation through attention |[paper](https://arxiv.org/pdf/2012.12877.pdf) [code](https://github.com/facebookresearch/deit)  |__arXiv__|Facebook AI|15 Jan 2021|
|5|Pyramid Vision Transformer |Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions|[paper](https://arxiv.org/abs/2102.12122) [code](https://github.com/whai362/PVT)  |__arXiv__|Nanjing University of Science and Technology|24 Feb 2021|
|6|TNT |Transformer in Transformer|[paper](https://arxiv.org/pdf/2103.00112.pdf) [code](https://github.com/huawei-noah/noah-research/tree/master/TNT)  |__arXiv__|Noah's Ark Lab|27 Feb 2021|
|7|PiT |Rethinking Spatial Dimensions of Vision Transformers|[paper](https://arxiv.org/pdf/2103.16302.pdf) [code](https://github.com/naver-ai/pit)  |__arXiv__|NAVER AI Lab|30 Mar 2021|
|8|T2T-ViT |Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet|[paper](https://arxiv.org/pdf/2101.11986.pdf) [code](https://github.com/yitu-opensource/T2T-ViT)  |__arXiv__| NUS|22 Mar 2021|
|9|CPVT |Conditional Positional Encodings for Vision Transformers|[paper](https://arxiv.org/pdf/2102.10882.pdf) [code](https://github.com/Meituan-AutoML/CPVT)  |__arXiv__| Meituan Inc|18 Mar 2021|
|10|ViL |Multi-Scale Vision Longformer:A New Vision Transformer for High-Resolution Image Encoding|[paper](https://arxiv.org/pdf/2103.15358.pdf)   |__arXiv__| Microsoft Corporation|29 Mar 2021|
|11|CoaT |Co-Scale Conv-Attentional Image Transformer|[paper](https://arxiv.org/abs/2104.06399) [code](https://github.com/mlpc-ucsd/CoaT)  |__arXiv__| University of California San Diego|13 April 2021|
|12|CoaT |Co-Scale Conv-Attentional Image Transformer|[paper](https://arxiv.org/abs/2104.06399) [code](https://github.com/mlpc-ucsd/CoaT)  |__arXiv__| University of California San Diego|13 April 2021|
|13|pruning |Visual Transforemr Pruning | [paper](https://arxiv.org/pdf/2104.08500.pdf) |__arXiv__|Zhejiang University| 17 April 2021 |
|14|pruning |Visual Transforemr Pruning | [paper](https://arxiv.org/pdf/2104.08500.pdf) |__arXiv__|Zhejiang University| 17 April 2021 |
|15|ViL| Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding| [paper](https://arxiv.org/pdf/2103.15358.pdf) |__arXiv__|Microsoft Corporation |29 Mar 2021|
|16|M2TR| M2TR: Multi-modal Multi-scale Transformersfor Deepfake Detection | [paper](https://arxiv.org/pdf/2104.09770.pdf) | __arXiv__ | Fudan Univeristy | 21 Apr|
|17|VisTransformer | Visformer: The Vision-friendly Transformer |[paper](https://arxiv.org/pdf/2104.12533.pdf) [code](https://github.com/danczs/Visformer) | __arXiv__ | Beihang University | 26 April 2021|
|18| ConTNet | ConTNet: Why not use convolution and transformer at the same time?| [paper](https://arxiv.org/pdf/2104.13497.pdf) [code](https://github.com/yan-hao-tian/ConTNet)|__arXiv__| ByteDance AI Lab | 27 Apr 2021 |
|19| Twins-SVT | Twins: Revisiting the Design of Spatial Attention in Vision Transformers  | [paper](https://arxiv.org/pdf/2104.13840.pdf) [code](https://github.com/Meituan-AutoML/Twins) |__arXiv__ | Meituan Inc | 28 Apr 2021|
|20|LeViT| LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference| [paper](https://arxiv.org/pdf/2104.01136.pdf) [code](https://github.com/facebookresearch/LeViT) | __arXiv__ | Facebook | 6 May 2021 |

# Viusal Relationship Detection
|No.  |Model Name |Title |Links |Pub. | Organization| Release Time | 
|-----|:-----:|:-----:|:-----:|:--------:|:---:|:-------:|
|1| RelTransformer | RelTransformer: Balancing the Visual Relationship Detection from Local Context, Scene and Memory| [paper](https://arxiv.org/pdf/2104.11934.pdf) [code](https://github.com/Vision-CAIR/RelTransformer) | __arXiv__| KAUST| 24 April 2021|



## Common Datasets
|No. |Dataset Name |Download Link|
|-----|:-----:|:-----:|
