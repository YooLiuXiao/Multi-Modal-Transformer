# Image & Language (Retrieval & captioning & image generation )

|No.  |Model Name |Title |Links |Pub. | Organization| Release Time | 
|-----|:-----:|:-----:|:-----:|:--------:|:---:|:-------:|
|1|ViusalGPT |VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning |[paper]( https://arxiv.org/abs/2102.10407) [code]( https://github.com/Vision-CAIR/VisualGPT) |__arXiv 2021__|KAUST|20 Feb 2021|
|2|Kaleido-BERT |Kaleido-BERT: Vision-Language Pre-training on Fashion Domain |[paper](https://arxiv.org/pdf/2103.16110.pdf) [code]( https://github.com/mczhuge/Kaleido-BERT/) |__CVPR 2021__|AliBaba Group|15 April 2021|
|3|CLIPBERT |Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling | [paper](https://arxiv.org/pdf/2102.06183.pdf) [code](https://github.com/jayleicn/ClipBERT) |__CVPR 2021__| UNC | 11 Feb 2021|
|4| -|Probabilistic Embeddings for Cross-Modal Retrieval| [paper](https://openaccess.thecvf.com/content/CVPR2021/html/Chun_Probabilistic_Embeddings_for_Cross-Modal_Retrieval_CVPR_2021_paper.html) [github](https://github.com/naver-ai/pcme) | __CVPR 2021__ | NAVER Lab|  14 June 2021|
|5| -| Scaling Up Vision-Language Representation Learning With Noisy Text Supervision | [paper](https://arxiv.org/pdf/2102.05918.pdf) | ICML 2021| Google | 11 June 2021|
|6|-|Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training| [paper](https://arxiv.org/pdf/2106.13488.pdf) | arXiv| MSRA| 28 June 2021|
|7| CogView| CogView: Mastering Text-to-Image Generation via Transformers | [paper](https://arxiv.org/pdf/2105.13290.pdf) [code](https://github.com/THUDM/CogView) | arXiv | TsingHua University | 28 May 2021| 
|8|ViLT| ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision| [paper](https://arxiv.org/pdf/2102.03334.pdf) [code](https://github.com/dandelin/vilt) | ICML 2021 | NAVER AI lab|  10 Jun 2021| 
|9| - |Unifying Vision-and-Language Tasks via Text Generation | [paper](https://arxiv.org/pdf/2102.02779.pdf) [code](https://github.com/j-min/VL-T5) | ICML 2021 | UNC | 23 May 2021|
|10| Pixel-BERT | Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers | [paper](https://arxiv.org/pdf/2004.00849.pdf) | arXiv | Univesity of Science and Technology Beijing |  22 Jun 2020 |
|11| -| How Much Can CLIP Benefit Vision-and-Language Tasks?| [paper](https://arxiv.org/pdf/2107.06383.pdf)| arXiv| UCB | 13 Jul 2021|
|12| LXMERT |LXMERT: Learning Cross-Modality Encoder Representations from Transformers| [paper](https://arxiv.org/abs/1908.07490) [code](https://github.com/airsplay/lxmert)| EMNLP 2019| UNC Chapel Hill | 3 Dec 2019|
|13| ViLBERT | VilBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks| [paper](https://arxiv.org/abs/1908.02265) [code](https://github.com/jiasenlu/vilbert_beta)| NeurIPS 2019| Georgia Institute of Technology | 6 Aug 2019|
|14| ImageBERT | ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data | [paper](https://arxiv.org/abs/2001.07966) | arXiv | Bing, Microsoft|23 Jan 2020|
|15| Unicoder-VL | Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training | [paper](https://arxiv.org/pdf/1908.06066v3.pdf) | AAAI 2020| MSRA | 2 Dec 2019|
|16| VLP | Unified Vision-Language Pre-Training for Image Captioning and VQA | [paper](https://arxiv.org/pdf/1909.11059.pdf) [code](https://github.com/LuoweiZhou/VLP) | AAAI 2020| University of Michigan | 4 Dec 2019|
|17| XGPT |XGPT: Cross-modal Generative Pre-Training for Image Captioning |[paper](https://arxiv.org/pdf/2003.01473.pdf) | arXiv| Peking University | 4 Mar 2020|
|18| 12-IN-1 | 12-in-1: Multi-Task Vision and Language Representation Learning | [paper](https://arxiv.org/pdf/1912.02315.pdf) [code](https://github.com/facebookresearch/vilbert-multi-task) | CVPR 2020 | Facebook | 5 Dec 2019|
|19| FashionBERT | FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval | [paper](https://arxiv.org/abs/2005.09801)| SIGIR | Alibaba | 20 May 2020|
|20| UNITER | UNITER: UNiversal Image-TExt Representation Learning | [paper](https://arxiv.org/abs/1909.11740) [code](https://github.com/ChenRocks/UNITER) | ECCV 2020 | Microsoft Dynamics 365 AI Research| 25 Sep 2019|
|21| VisDial-BERT | Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline | [paper](https://arxiv.org/abs/1912.02379) [code](https://github.com/vmurahari3/visdial-bert) | ECCV 2020 | 1Georgia Institute of Technology | 31 Mar 2020 |
|22| OSCAR | Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks | [paper](https://arxiv.org/abs/2004.06165) [code](https://github.com/microsoft/Oscar) | ECCV 2020| Microsoft |13 Apr 2020|
|23| KD-VLP| KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation |[paper](https://arxiv.or/pdf/2109.10504v1.pdf) | arXiv | ShanghaiTech | 22 Sep 2021|
|24| Fast & Slow| Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers|[paper](https://arxiv.org/abs/2103.16553)| CVPR 2021 |DeepMind | 30 Mar 2021|
|25| - |Unifying Multimodal Transfomer for Bi-directional Image and Text Generation | [paper](https://arxiv.org/abs/2110.09753) | Arxiv | Sun Yat-sen University| 19 Oct 2021 |
|26|SOHO| Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning | [paper](https://arxiv.org/pdf/2104.03135.pdf) | CVPR 2021 | University of Science and Technology Beijing | 8 Apr 2021|
|27| E2E-VLP | E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual| [paper](https://aclanthology.org/2021.acl-long.42.pdf) | ACL 2021| Alibaba Group | 3 June 2021|
|28| KD-VLP | KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation | [paper](https://arxiv.org/abs/2109.10504) |EMNLP 2021| ShanghaiTech| 22 Sep 2021|
|29| L-Verse| L-Verse: Bidirectional Generation Between Image and Text| [paper](https://arxiv.org/pdf/2111.11133.pdf) | ArXiv | LG AI Research | 22 Nov 2021|




# Object Detection

|No.  |Model Name |Title |Links |Pub. | Organization| Release Time | 
|-----|:-----:|:-----:|:-----:|:--------:|:---:|:-------:|
|1|MDTER|  MDETR - Modulated Detection for End-to-End Multi-Modal Understanding   | [paper](https://arxiv.org/pdf/2104.12763.pdf)  [code](https://github.com/ashkamath/mdetr)  | __ICCV 2021__|NYU |26 April 2021|
|2| pix2seq| pix2seq: A Language Modeling Framework for Object Detection| [paper](https://arxiv.org/abs/2109.10852) | arXiv | Google Research |  22 Sep 2021 |




